---
title: "Review of the algorithm of a Gibbs Sampler"
author:
- name: Andrew MacDonald
output:
  distill::distill_article:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(library(tidyverse))
```

Minerva and Athena quote from McElreath

putting things in boxes

* conjugacy 
* 


## Conjugate prior review

In bayesian statistics we use probability distributions to measure uncertainty about the parameter values we care about. 

classic example is the binomial and beta distributions 

(proof on the board??)

## Seed survival part I 

Let's imagine an experiment where we place exactly 42 seeds in experimental plots. We use 30 experimental plots.

We want to estimate a probability of germination for these seedlings

Our model looks like this:

$$
\begin{align}
y &\sim \text{Binomial}(p_{\text{survival}}, N)\\
p_{\text{survival}} &\sim \text{Beta}(2,5)\\
\end{align}
$$

in English, this means:

* We thing that the number of seedlings which germinate will follow a Binomial distribution. This means that out of a total of 42, some survive. The probability for any surviving is the same: $p_{\text{survival}}$
* 

$$
[p_{\text{survival}} | ]
$$


$$

$$



```{r}
## start with some survival data -- out of 42
true_prob <- 0.7

set.seed(1859)
n_surviving <- rbinom(n = 300, size = 42, prob = true_prob)

hist(n_surviving, xlim = c(0, 42))
```

Can use the conjugate posterior.

First, use the conjugate prior to the binomial (beta distribution).

```{r}
curve(dbeta(x, 2,5))
```

This might be a reasonable first guess. but what does it actually mean? we need to do predictions to make sure.

```{r}
possible_probabilities <- rbeta(300, 2, 5)

hist(rbinom(30, 42, possible_probabilities), xlim = c(0, 42))

```
Do this a bunch of times, to see if the prior is foolish or not.

With enough data, even an outlandish prior is fine: 

```{r}
lived <- sum(n_surviving)
died <- sum(42 - n_surviving)
```

calculate the posterior

```{r}
curve(dbeta(x, 2, 5),ylim = c(0,4))
curve(dbeta(x, 2 + lived, 5 + died), add = TRUE, lty = 2)
abline(v = true_prob)

abline(v =(2 + lived)/(2 + lived + 5 + died), col = "red")
```

But what if we *didn't* want to do this?

## Metropolis

Suppose we had written the model like this:

$$
\begin{align}
y &\sim \text{Binomial}(p, N) \\
logit(p) &\sim \text{Normal}(1, 1)
\end{align}
$$

or could also be said:

$$
\begin{align}
y &\sim {n \choose k} \left(\frac{e^\alpha}{1 + e^\alpha} \right)^{k}\left(1-\frac{e^\alpha}{1 + e^\alpha}\right)^{n-k} \\
\alpha &\sim \text{Normal}(1, 1)
\end{align}
$$

Now we have no conjugate posterior

```{r}

## define likelihoods for our problem
binomial_loglike <- function(data_vec, prob_new){
  sum(dbinom(data_vec, size = 42, 
             prob = prob_new, log = TRUE))
}

normal_loglike <- function(val, mean_prior, sd_prior){
  dnorm(val, mean = mean_prior, sd = sd_prior, log = TRUE)
}

logistic_curve <- function(x) exp(x)/(1 + exp(x))

## function to use these to calculate the value of the numerator in Bayes Formula
numerator <- function(v, data)  {
  binomial_loglike(data_vec = data, prob_new = logistic_curve(v)) + 
    normal_loglike(val = v, mean = -1, sd = 2)
}

# proposal function to make new parameter values.
propose_new <- function(x, sig_tune) rnorm(1, mean = x, sd = sig_tune)

# start value
metropolis <- function(dataset,
                       n_samples,
                       tune_parameter,
                       start_value, 
                       numerator_function = numerator){
  chain <- numeric(n_samples)
  chain[1] <- start_value
  
  for (i in 2:length(chain)){
    
    start <- chain[i - 1]
    
    new <- propose_new(start, sig_tune = tune_parameter)
    
    r <- exp(numerator_function(v = new, data = dataset) - 
               numerator_function(v = start, data = dataset))
    
    p_accept <- min(1, r)
    
    chain[i] <- ifelse(runif(1) < p_accept, new, start)
  }
  return(chain)
}


chain <- metropolis(dataset = n_surviving,
                    n_samples = 3000,
                    tune_parameter = 0.2, 
                    start_value = 0)

plot(chain[300:3000], type = "l")
```

```{r}
hist(chain[300:5000])
```


```{r}
hist(plogis(chain[200:5000]), xlim = c(0.6,.9), probability = TRUE)
abline(v = true_prob)

curve(dbeta(x, 2 + lived, 5 + died), add = TRUE, lty = 2)
```


Rejection

<make a plot of rejections?

```{r}
rejects <- zapsmall(chain[2:length(chain)] - chain[1:(length(chain) - 1)] )

plot(rejects)

sum(rejects == 0)  / length(chain)
```

lots of rejection of samples. That's not very efficient! 

## Metropolis-Hastings 

The same, but we can use an asymmetrical proposal distribution.

<example??>

## Gibbs sampling

<div style="width:100%;height:0;padding-bottom:61%;position:relative;"><iframe src="https://giphy.com/embed/3oEhmHmWP3Y9wQxoli" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div><p><a href="https://giphy.com/gifs/funny-lol-cartoon-3oEhmHmWP3Y9wQxoli">via GIPHY</a></p>

Sample each parameter, keeping the others fixed. Let's consider just two for now:

Start with random values for each parameter

assuming that all are correct except the first, sample a value for the first

Assuming that sample was correct for the first, what is the second? 

<cartoon image here> 

```{r}
true_mean <- 31
true_sd <-  4
set.seed(1859)
mass_values <- rnorm(30, mean = true_mean, sd = true_sd)
```

define the model, but in a deliberate way, choosing the priors to be conjugates

$$
\begin{align}
y &\sim \text{Normal}(\mu, \sigma) \\
\mu &\sim \text{Normal}(10,5) \\
\frac{1}{\sigma^2} &\sim \text{Gamma}(3^2/2^2, 3/2^2)
\end{align}
$$

```{r}

rg <- rgamma(1000, shape = 3^2/2^2, rate = 3/2^2)
mean(rg)
sd(rg)

curve(dgamma(x, shape = 3^2/2^2, rate = 3/2^2), xlim = c(0, 5))
```

if the variance is known, the posterior for the mean is a normal distribution:

$$
\mu' \sim \text{Normal}\left(\frac{\frac{10}{5}+ \frac{\sum{y}}{\sigma^2}}{\frac{1}{5^2} + \frac{n}{\sigma^2}}, \left(\frac{1}{5^2} + \frac{n}{\sigma^2}\right)\right)
$$

Note that $10$ is the prior mean and $5$ is the prior variance for $\mu$, the mean of the distribution.

if the mean is known, then the posterior distribution for the variance is an inverse gamma:

$$
\frac{1}{\sigma^2} \sim \text{Gamma}\left(\alpha + \frac{n}{2}, 
\beta + \frac{\sum (y_i - \mu)^2}{2}\right)
$$




## Clara Coffee Example

```{r}
clara_coffee <- tibble::tribble(
  ~Coffee_extraction, ~time,
  15.88,              19.13875598,
  16.72,              7.633587786,
  16.72,              22.90076336,
  17,                       30,
  17.3,              5.479452055,
  17.64,              12.93103448,
  17.75,              9.459459459,
  17.77,              12.93103448,
  17.99,              10.18867925,
  18,                        6,
  18.03,              11.86440678,
  18.32,              15.54404145,
  18.57,              23.71794872,
  18.89,              24.75247525,
  19.13,                      6.4,
  19.33,              20.30456853,
  19.36,              18.71345029,
  19.5,              33.49282297,
  19.67,              13.61867704,
  20.23,              15.81027668,
  20.26,              17.54385965,
  20.39,              13.76146789,
  20.45,              14.93775934,
  20.57,              13.61867704,
  20.61,              14.85148515,
  20.69,                39.408867,
  20.76,              6.944444444,
  20.84,              8.968609865,
  20.88,              11.40684411,
  20.95,              9.756097561,
  21.04,              4.843304843,
  21.2,               14.0969163,
  21.25,                     15.2,
  21.36,              6.060606061,
  21.72,              11.52073733,
  21.86,              10.89494163,
  22.14,              8.849557522,
  22.26,              8.532423208,
  22.47,              13.39285714,
  22.48,              5.333333333,
  23.48,              6.134969325,
  24.29,              6.993006993,
  24.35,               7.86163522,
  24.63,              12.95336788
)


library(ggplot2)

ggplot(clara_coffee, aes(x = time, y = Coffee_extraction/100)) +
  geom_point()

#' what are the response distribution?
#'
#' what is the question?

#' quel est la relation entre la coulÃ©e d'eau et lextraction de cafe
#'

# response distribtuion -- between 0 and 1. Could be the beta.

# normal -- for pragmatic reasons ?

curve(dgamma(x, shape = (2)^2/4, rate = 2/4), xlim = c(0,5))
sigma_coffee <- 1/sqrt(rgamma(1, shape = (2)^2/4, rate = 2/4))
mean_coffee <- rnorm(1, 21, 3)
change_coffee <- rnorm(1, 1, 1)

clara_coffee$t_centered <- with(clara_coffee, time - mean(time))

clara_coffee_prior <- transform(clara_coffee,
                                average_measure = mean_coffee +
                                  change_coffee*t_centered)

clara_coffee_prior$fake_obs <- rnorm(n = nrow(clara_coffee),
                                     mean = clara_coffee_prior$average_measure,
                                     sd = sigma_coffee)


ggplot(clara_coffee_prior, aes(x = Coffee_extraction, y = fake_obs)) +
  geom_point()



```


```{r eval=FALSE}
library(brms)

coffee_formula <- bf(Coffee_extraction ~ t_centered)

get_prior(coffee_formula, data = clara_coffee_prior)

cafe_stan <- brm(Coffee_extraction ~ t_centered,
                     data = clara_coffee_prior, )

```

